diff --git a/configs/base_gs.yaml b/configs/base_gs.yaml
index b9ae82c..31dfea7 100644
--- a/configs/base_gs.yaml
+++ b/configs/base_gs.yaml
@@ -66,7 +66,7 @@ model:
 
   background:
     name: background-color # one of [skip-background, background-color]
-    color: black # one of [black, white, random] needs to be defined if name == background-color
+    color: random # one of [black, white, random] needs to be defined if name == background-color
 
   print_stats: true
 
@@ -125,3 +125,11 @@ hydra:
     chdir: false
   run:
     dir: .
+
+# Bilateral grid parameters
+bilateral_grid:
+  enabled: true
+  lr: 0.002
+  grid_X: 16
+  grid_Y: 16
+  grid_W: 8
diff --git a/configs/base_mcmc.yaml b/configs/base_mcmc.yaml
index 188becb..f2ef980 100644
--- a/configs/base_mcmc.yaml
+++ b/configs/base_mcmc.yaml
@@ -16,3 +16,14 @@ loss:
 
   use_scale: true
   lambda_scale: 0.01
+
+checkpoint:
+  iterations: ${int_list:[ 7000, 30000 ]}
+
+# Bilateral grid parameters
+bilateral_grid:
+  enabled: true
+  lr: 0.002
+  grid_X: 16
+  grid_Y: 16
+  grid_W: 8
diff --git a/configs/dataset/colmap.yaml b/configs/dataset/colmap.yaml
index 1942f5e..2d7cf5f 100644
--- a/configs/dataset/colmap.yaml
+++ b/configs/dataset/colmap.yaml
@@ -1,3 +1,3 @@
 type: colmap
-downsample_factor: 1
-test_split_interval: 8
\ No newline at end of file
+downsample_factor: 2
+test_split_interval: 8
diff --git a/configs/strategy/mcmc.yaml b/configs/strategy/mcmc.yaml
index e2bd14a..e5ba79c 100644
--- a/configs/strategy/mcmc.yaml
+++ b/configs/strategy/mcmc.yaml
@@ -2,7 +2,7 @@ method: MCMCStrategy
 
 print_stats: true
 binom_n_max: 51 # Default value from MCMC paper (no good reason, just a large number https://github.com/ubc-vision/3dgs-mcmc/issues/8)
-opacity_threshold: 0.005 # Minimum opacity -> used to dermine if a Gaussian is alive or dead
+opacity_threshold: 0.05 # Minimum opacity -> used to dermine if a Gaussian is alive or dead
 
 relocate:
   start_iteration: 500 # Start applying the strategy after start_iteration training iterations
diff --git a/downscale.py b/downscale.py
new file mode 100644
index 0000000..2f02865
--- /dev/null
+++ b/downscale.py
@@ -0,0 +1,57 @@
+#!/usr/bin/env python3
+"""
+fast_downscale.py
+
+python fast_downscale.py /workspace/GreenTrees/Plot123/images 2
+"""
+
+import sys, os
+from pathlib import Path
+from concurrent.futures import ThreadPoolExecutor as TPE
+from functools import partial
+from PIL import Image, ImageOps
+from tqdm import tqdm
+import multiprocessing as mp
+
+IMAGE_EXTS = {".jpg", ".jpeg", ".png", ".tif", ".tiff", ".bmp", ".webp"}
+
+def _downscale_one(dst_dir: Path, factor: int, img_path: Path) -> None:
+    with Image.open(img_path) as im:
+        im = ImageOps.exif_transpose(im)           # honour orientation
+        w, h = im.size
+        im_ds = im.resize((w // factor, h // factor), Image.Resampling.LANCZOS)
+        im_ds.save(dst_dir / img_path.name, quality=95, subsampling=0)
+
+def main() -> None:
+    if len(sys.argv) != 3:
+        sys.exit("Usage: fast_downscale.py <image_folder> <factor:int>")
+    src = Path(sys.argv[1]).expanduser().resolve()
+    if not src.is_dir():
+        sys.exit(f"❌  {src} is not a directory")
+    factor = int(sys.argv[2])
+    if factor < 1:
+        sys.exit("❌  factor must be ≥1")
+
+    if factor == 1:
+        print("Factor is 1; nothing to do."); return
+
+    dst = src.parent / f"{src.name}_{factor}"
+    dst.mkdir(exist_ok=True)
+
+    images = [p for p in src.iterdir() if p.suffix.lower() in IMAGE_EXTS]
+    if not images:
+        sys.exit("❌  No supported images found")
+
+    worker = partial(_downscale_one, dst, factor)
+    n_threads = min(len(images), mp.cpu_count() * 2)   # I/O + CPU mix
+
+    with TPE(max_workers=n_threads) as pool:
+        list(tqdm(pool.map(worker, images),
+                  total=len(images),
+                  desc=f"Resizing x{factor} → {dst.name}",
+                  unit="img"))
+
+    print(f"✅  Done. {len(images)} images written to {dst}")
+
+if __name__ == "__main__":
+    main()
\ No newline at end of file
diff --git a/requirements.txt b/requirements.txt
index c6f21ea..2f08156 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -20,10 +20,11 @@ dataclasses_json
 # JIT compilation
 setuptools <72.1.0
 # Fused-ssim
-git+https://github.com/rahul-goel/fused-ssim@1272e21a282342e89537159e4bad508b19b34157
+git+https://github.com/rahul-goel/fused-ssim@8bdb59feb7b9a41b1fab625907cb21f5417deaac
 # Playground
 tqdm
 libigl
 pygltflib
 # --find-links https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.1.2_cu118.html
-# kaolin==0.17.0
\ No newline at end of file
+# kaolin==0.17.0
+tensorly
\ No newline at end of file
diff --git a/threedgrut/datasets/__init__.py b/threedgrut/datasets/__init__.py
index 8fbc887..d47bd6b 100644
--- a/threedgrut/datasets/__init__.py
+++ b/threedgrut/datasets/__init__.py
@@ -33,12 +33,14 @@ def make(name: str, config, ray_jitter):
                 bg_color=config.model.background.color,
             )
         case "colmap":
+            holdout = getattr(config.dataset, "holdout_every", 8)
             train_dataset = ColmapDataset(
                 config.path,
                 split="train",
                 downsample_factor=config.dataset.downsample_factor,
                 test_split_interval=config.dataset.test_split_interval,
                 ray_jitter=ray_jitter,
+                holdout_every=holdout,
             )
             val_dataset = ColmapDataset(
                 config.path,
@@ -47,6 +49,7 @@ def make(name: str, config, ray_jitter):
                 test_split_interval=config.dataset.test_split_interval,
             )
         case "scannetpp":
+            holdout = getattr(config.dataset, "holdout_every", 8)
             train_dataset = ScannetppDataset(
                 config.path,
                 split="train",
diff --git a/threedgrut/datasets/dataset_colmap.py b/threedgrut/datasets/dataset_colmap.py
index 457a48d..25ad79a 100644
--- a/threedgrut/datasets/dataset_colmap.py
+++ b/threedgrut/datasets/dataset_colmap.py
@@ -282,6 +282,7 @@ class ColmapDataset(Dataset, BoundedMultiViewDataset, DatasetVisualization):
         return self.scene_bbox
 
     def get_scene_extent(self):
+        logger.info(f"ColmapDataset scene extent: {self.cameras_extent}")
         return self.cameras_extent
 
     def get_observer_points(self):
@@ -302,6 +303,7 @@ class ColmapDataset(Dataset, BoundedMultiViewDataset, DatasetVisualization):
             "data": torch.tensor(image_data).reshape(out_shape),
             "pose": torch.tensor(self.poses[idx]).unsqueeze(0),
             "intr": self.get_intrinsics_idx(idx),
+            "image_id": idx,
         }
 
         # Only add mask to dictionary if it exists
@@ -319,6 +321,8 @@ class ColmapDataset(Dataset, BoundedMultiViewDataset, DatasetVisualization):
         data = batch["data"][0].to(self.device, non_blocking=True) / 255.0
         pose = batch["pose"][0].to(self.device, non_blocking=True)
         intr = batch["intr"][0].item()
+        # batch["image_id"] is collated as tensor shape (1,), move to device and keep shape [1]
+        image_id = batch["image_id"].to(self.device, non_blocking=True).long()
         assert data.dtype == torch.float32
         assert pose.dtype == torch.float32
 
@@ -330,6 +334,7 @@ class ColmapDataset(Dataset, BoundedMultiViewDataset, DatasetVisualization):
             "rays_dir": rays_dir,
             "T_to_world": pose,
             f"intrinsics_{camera_name}": camera_params_dict,
+            "image_id": image_id,
         }
 
         if "mask" in batch:
diff --git a/threedgrut/datasets/dataset_nerf.py b/threedgrut/datasets/dataset_nerf.py
index f828a8c..6832f12 100644
--- a/threedgrut/datasets/dataset_nerf.py
+++ b/threedgrut/datasets/dataset_nerf.py
@@ -178,6 +178,7 @@ class NeRFDataset(Dataset, BoundedMultiViewDataset, DatasetVisualization):
         output_dict = {
             "data": torch.tensor(img).reshape(out_shape),
             "pose": torch.tensor(self.poses[idx]).unsqueeze(0),
+            "image_id": idx,
         }
 
         if os.path.exists(mask_path := self.mask_paths[idx]):
@@ -202,6 +203,7 @@ class NeRFDataset(Dataset, BoundedMultiViewDataset, DatasetVisualization):
             "rays_dir": self.rays_d_cam,
             "T_to_world": pose,
             "intrinsics": self.intrinsics,
+            "image_id": batch["image_id"].to(self.device, non_blocking=True).long(),
         }
 
         if "mask" in batch:
diff --git a/threedgrut/datasets/protocols.py b/threedgrut/datasets/protocols.py
index 0698424..12bcea4 100644
--- a/threedgrut/datasets/protocols.py
+++ b/threedgrut/datasets/protocols.py
@@ -32,6 +32,7 @@ class Batch:
     intrinsics: Optional[list] = None
     intrinsics_OpenCVPinholeCameraModelParameters: Optional[dict] = None
     intrinsics_OpenCVFisheyeCameraModelParameters: Optional[dict] = None
+    image_id: Optional[torch.Tensor] = None  # Unique identifier for the originating image (scalar long tensor)
 
     def __post_init__(self):
         batch_size = self.T_to_world.shape[0]
diff --git a/threedgrut/model/model.py b/threedgrut/model/model.py
index 415c3b0..b2b3669 100644
--- a/threedgrut/model/model.py
+++ b/threedgrut/model/model.py
@@ -575,7 +575,12 @@ class MixtureOfGaussians(torch.nn.Module):
         Returns:
             A dictionary containing the output of the model
         """
-        return self.renderer.render(self, batch, train, frame_id)
+        outputs = self.renderer.render(self, batch, train, frame_id)
+
+        # Color correction is now handled inside the renderer (before background) to ensure guidance colors are not
+        # biased by random background. Avoid applying it a second time here.
+
+        return outputs
 
     def trace(self, rays_o, rays_d, T_to_world=None):
         """ Traces the model with the given rays. This method is a convenience method for ray-traced inference mode.
diff --git a/threedgrut/trainer.py b/threedgrut/trainer.py
index 864a751..adbbcda 100644
--- a/threedgrut/trainer.py
+++ b/threedgrut/trainer.py
@@ -43,6 +43,10 @@ from threedgrut.utils.timer import CudaTimer
 from threedgrut.utils.misc import jet_map, create_summary_writer, check_step_condition
 from threedgrut.optimizers import SelectiveAdam
 
+# Bilateral grid utilities
+from threedgrut.utils.lib_bilagrid import BilateralGrid, total_variation_loss, slice as bilagrid_slice
+import math
+
 class Trainer3DGRUT:
     """Trainer for paper: "3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes" """
 
@@ -73,6 +77,11 @@ class Trainer3DGRUT:
     tracking: Dict
     """ Contains all components used to report progress of training """
 
+    # --- Bilateral grid related attributes ---
+    bil_grids: Optional[BilateralGrid] = None
+    bil_grid_optimizers: list[torch.optim.Optimizer] = []
+    bil_grid_schedulers: list[torch.optim.lr_scheduler.LRScheduler] = []
+
     @staticmethod
     def create_from_checkpoint(resume: str, conf: DictConfig):
         """Create a new trainer from a checkpoint file"""
@@ -126,6 +135,12 @@ class Trainer3DGRUT:
         self.init_scene_extents(self.train_dataset)
         logger.log_rule("Initialize Model")
         self.init_model(conf, self.scene_extent)
+
+        # ---------------------------------------------------
+        # Initialize bilateral grids – one grid per training image
+        # ---------------------------------------------------
+        self._init_bilateral_grids()
+
         self.init_densification_and_pruning_strategy(conf)
         logger.log_rule("Setup Model Weights & Training")
         self.init_metrics()
@@ -169,6 +184,7 @@ class Trainer3DGRUT:
     def init_scene_extents(self, train_dataset: BoundedMultiViewDataset) -> None:
         scene_bbox: tuple[torch.Tensor, torch.Tensor]  # Tuple of vec3 (min,max)
         scene_extent = train_dataset.get_scene_extent()
+        logger.info(f"Trainer: scene_extent={scene_extent}")
         scene_bbox = train_dataset.get_scene_bbox()
         self.scene_extent = scene_extent
         self.scene_bbox = scene_bbox
@@ -395,6 +411,9 @@ class Trainer3DGRUT:
             rgb_gt = rgb_gt * mask
             rgb_pred = rgb_pred * mask
 
+        # Bilateral grid correction is applied inside model.forward so that `outputs['pred_rgb']`
+        # already contains corrected colors. No need to re‑apply it here.
+
         # L1 loss
         loss_l1 = torch.zeros(1, device=self.device)
         lambda_l1 = 0.0
@@ -437,9 +456,28 @@ class Trainer3DGRUT:
                 loss_scale = torch.abs(self.model.get_scale()).mean()
                 lambda_scale = self.conf.loss.lambda_scale
 
+        # Add bilateral grid total variation loss if enabled
+        tv_loss = torch.zeros(1, device=self.device)
+        if self.bil_grids is not None:
+            tv_loss = 10.0 * total_variation_loss(self.bil_grids.grids)
+
         # Total loss
-        loss = lambda_l1 * loss_l1 + lambda_ssim * loss_ssim + lambda_opacity * loss_opacity + lambda_scale * loss_scale
-        return dict(total_loss=loss, l1_loss=lambda_l1 * loss_l1, l2_loss=lambda_l2 * loss_l2, ssim_loss=lambda_ssim * loss_ssim, opacity_loss=lambda_opacity * loss_opacity, scale_loss=lambda_scale * loss_scale)
+        loss = (
+            lambda_l1 * loss_l1
+            + lambda_ssim * loss_ssim
+            + lambda_opacity * loss_opacity
+            + lambda_scale * loss_scale
+            + tv_loss
+        )
+        return dict(
+            total_loss=loss,
+            l1_loss=lambda_l1 * loss_l1,
+            l2_loss=lambda_l2 * loss_l2,
+            ssim_loss=lambda_ssim * loss_ssim,
+            opacity_loss=lambda_opacity * loss_opacity,
+            scale_loss=lambda_scale * loss_scale,
+            tv_loss=tv_loss,
+        )
 
     @torch.cuda.nvtx.range("log_validation_iter")
     def log_validation_iter(
@@ -728,18 +766,25 @@ class Trainer3DGRUT:
                     step=global_step, scene_extent=self.scene_extent, train_dataset=self.train_dataset, batch=gpu_batch, writer=self.tracking.writer
                 )
 
-            # Optimizer step
-            with torch.cuda.nvtx.range(f"train_{global_step}_backprop"):
-                if isinstance(model.optimizer, SelectiveAdam):
-                    assert outputs['mog_visibility'].shape == model.density.shape, f"Visibility shape {outputs['mog_visibility'].shape} does not match density shape {model.density.shape}"
-                    model.optimizer.step(outputs['mog_visibility'])
-                else:
-                    model.optimizer.step()
-                model.optimizer.zero_grad()
+            # First, update bilateral grid parameters if they exist
+            if self.bil_grids is not None:
+                for opt in self.bil_grid_optimizers:
+                    opt.step()
+                    opt.zero_grad(set_to_none=True)
+
+            if isinstance(model.optimizer, SelectiveAdam):
+                assert outputs['mog_visibility'].shape == model.density.shape, f"Visibility shape {outputs['mog_visibility'].shape} does not match density shape {model.density.shape}"
+                model.optimizer.step(outputs['mog_visibility'])
+            else:
+                model.optimizer.step()
+            model.optimizer.zero_grad()
 
             # Scheduler step
             with torch.cuda.nvtx.range(f"train_{global_step}_scheduler"):
                 model.scheduler_step(global_step)
+                if self.bil_grids is not None:
+                    for sch in self.bil_grid_schedulers:
+                        sch.step()
 
             # Post backward strategy step
             with torch.cuda.nvtx.range(f"train_{global_step}_post_opt_step"):
@@ -880,3 +925,35 @@ class Trainer3DGRUT:
             self.gui.training_done = True
             logger.info(f"🎨 GUI Blocking... Terminate GUI to Stop.")
             self.gui.block_in_rendering_loop(fps=60)
+
+    def _init_bilateral_grids(self):
+        """Create bilateral grids and associated optimizers & schedulers."""
+        # If config does not enable bilagrid training skip
+        if not (hasattr(self.conf, "bilateral_grid") and getattr(self.conf.bilateral_grid, "enabled", False)):
+            return
+
+        n_views = len(self.train_dataset)
+        # Grid resolution parameters – allow override via config
+        grid_X = getattr(self.conf.bilateral_grid, "grid_X", 16)
+        grid_Y = getattr(self.conf.bilateral_grid, "grid_Y", 16)
+        grid_W = getattr(self.conf.bilateral_grid, "grid_W", 8)
+
+        logger.info(f"Initializing bilateral grids: {n_views} views, res=({grid_X},{grid_Y},{grid_W})")
+        self.bil_grids = BilateralGrid(num=n_views, grid_X=grid_X, grid_Y=grid_Y, grid_W=grid_W).to(self.device)
+
+        # Reference implementation uses 2e‑3 * sqrt(batch_size) as base LR
+        bsz = 1  # Current pipeline uses batch_size=1
+        base_lr = self.conf.bilateral_grid.lr * math.sqrt(bsz)
+        optimizer = torch.optim.Adam(self.bil_grids.parameters(), lr=base_lr, eps=1e-15)
+        self.bil_grid_optimizers.append(optimizer)
+
+        # Expose the bilateral grids to the gaussian model so that the renderer can access it.
+        # This allows color‑correction to be applied during any forward rendering pass (e.g. GUI, inference).
+        setattr(self.model, "bil_grids", self.bil_grids)
+
+        from torch.optim.lr_scheduler import ChainedScheduler, LinearLR, ExponentialLR
+
+        warmup = LinearLR(optimizer, start_factor=0.01, total_iters=1000)
+        decay = ExponentialLR(optimizer, gamma=(0.01) ** (1.0 / self.conf.n_iterations))
+        scheduler = ChainedScheduler([warmup, decay])
+        self.bil_grid_schedulers.append(scheduler)
diff --git a/threedgrut/utils/lib_bilagrid.py b/threedgrut/utils/lib_bilagrid.py
new file mode 100644
index 0000000..f483573
--- /dev/null
+++ b/threedgrut/utils/lib_bilagrid.py
@@ -0,0 +1,280 @@
+# # Copyright 2024 Yuehao Wang (https://github.com/yuehaowang). This part of code is borrowed form ["Bilateral Guided Radiance Field Processing"](https://bilarfpro.github.io/).
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""
+This is a standalone PyTorch implementation of 3D bilateral grid and CP-decomposed 4D bilateral grid.
+To use this module, you can download the "lib_bilagrid.py" file and simply put it in your project directory.
+
+For the details, please check our research project: ["Bilateral Guided Radiance Field Processing"](https://bilarfpro.github.io/).
+
+#### Dependencies
+
+In addition to PyTorch and Numpy, please install [tensorly](https://github.com/tensorly/tensorly).
+We have tested this module on Python 3.9.18, PyTorch 2.0.1 (CUDA 11), tensorly 0.8.1, and Numpy 1.25.2.
+
+#### Overview
+
+- For bilateral guided training, you need to construct a `BilateralGrid` instance, which can hold multiple bilateral grids
+  for input views. Then, use `slice` function to obtain transformed RGB output and the corresponding affine transformations.
+
+- For bilateral guided finishing, you need to instantiate a `BilateralGridCP4D` object and use `slice4d`.
+
+#### Examples
+
+- Bilateral grid for approximating ISP:
+    <a target="_blank" href="https://colab.research.google.com/drive/1tx2qKtsHH9deDDnParMWrChcsa9i7Prr?usp=sharing">
+    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
+
+- Low-rank 4D bilateral grid for MR enhancement:
+    <a target="_blank" href="https://colab.research.google.com/drive/17YOjQqgWFT3QI1vysOIH494rMYtt_mHL?usp=sharing">
+    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
+
+
+Below is the API reference.
+
+"""
+
+import tensorly as tl
+import torch
+import torch.nn.functional as F
+from torch import nn
+
+tl.set_backend("pytorch")
+
+
+def color_affine_transform(affine_mats, rgb):
+    """Applies color affine transformations.
+
+    Args:
+        affine_mats (torch.Tensor): Affine transformation matrices. Supported shape: $(..., 3, 4)$.
+        rgb  (torch.Tensor): Input RGB values. Supported shape: $(..., 3)$.
+
+    Returns:
+        Output transformed colors of shape $(..., 3)$.
+    """
+    return (
+        torch.matmul(affine_mats[..., :3], rgb.unsqueeze(-1)).squeeze(-1)
+        + affine_mats[..., 3]
+    )
+
+
+def _num_tensor_elems(t):
+    return max(torch.prod(torch.tensor(t.size()[1:]).float()).item(), 1.0)
+
+
+def total_variation_loss(x):  # noqa: F811
+    """Returns total variation on multi-dimensional tensors.
+
+    Args:
+        x (torch.Tensor): The input tensor with shape $(B, C, ...)$, where $B$ is the batch size and $C$ is the channel size.
+    """
+    batch_size = x.shape[0]
+    tv = 0
+    for i in range(2, len(x.shape)):
+        n_res = x.shape[i]
+        idx1 = torch.arange(1, n_res, device=x.device)
+        idx2 = torch.arange(0, n_res - 1, device=x.device)
+        x1 = x.index_select(i, idx1)
+        x2 = x.index_select(i, idx2)
+        count = _num_tensor_elems(x1)
+        tv += torch.pow((x1 - x2), 2).sum() / count
+    return tv / batch_size
+
+
+def slice(bil_grids, xy, rgb, grid_idx):
+    """Slices a batch of 3D bilateral grids by pixel coordinates `xy` and gray-scale guidances of pixel colors `rgb`.
+
+    Supports 2-D, 3-D, and 4-D input shapes. The first dimension of the input is the batch size
+    and the last dimension is 2 for `xy`, 3 for `rgb`, and 1 for `grid_idx`.
+
+    The return value is a dictionary containing the affine transformations `affine_mats` sliced from bilateral grids and
+    the output color `rgb_out` after applying the afffine transformations.
+
+    In the 2-D input case, `xy` is a $(N, 2)$ tensor, `rgb` is  a $(N, 3)$ tensor, and `grid_idx` is a $(N, 1)$ tensor.
+    Then `affine_mats[i]` can be obtained via slicing the bilateral grid indexed at `grid_idx[i]` by `xy[i, :]` and `rgb2gray(rgb[i, :])`.
+    For 3-D and 4-D input cases, the behavior of indexing bilateral grids and coordinates is the same with the 2-D case.
+
+    .. note::
+        This function can be regarded as a wrapper of `color_affine_transform` and `BilateralGrid` with a slight performance improvement.
+        When `grid_idx` contains a unique index, only a single bilateral grid will used during the slicing. In this case, this function will not
+        perform tensor indexing to avoid data copy and extra memory
+        (see [this](https://discuss.pytorch.org/t/does-indexing-a-tensor-return-a-copy-of-it/164905)).
+
+    Args:
+        bil_grids (`BilateralGrid`): An instance of $N$ bilateral grids.
+        xy (torch.Tensor): The x-y coordinates of shape $(..., 2)$ in the range of $[0,1]$.
+        rgb (torch.Tensor): The RGB values of shape $(..., 3)$ for computing the guidance coordinates, ranging in $[0,1]$.
+        grid_idx (torch.Tensor): The indices of bilateral grids for each slicing. Shape: $(..., 1)$.
+
+    Returns:
+        A dictionary with keys and values as follows:
+        ```
+        {
+            "rgb": Transformed RGB colors. Shape: (..., 3),
+            "rgb_affine_mats": The sliced affine transformation matrices from bilateral grids. Shape: (..., 3, 4)
+        }
+        ```
+    """
+
+    sh_ = rgb.shape
+
+    grid_idx_unique = torch.unique(grid_idx)
+    if len(grid_idx_unique) == 1:
+        # All pixels are from a single view.
+        grid_idx = grid_idx_unique  # (1,)
+        xy = xy.unsqueeze(0)  # (1, ..., 2)
+        rgb = rgb.unsqueeze(0)  # (1, ..., 3)
+    else:
+        # Pixels are randomly sampled from different views.
+        if len(grid_idx.shape) == 4:
+            grid_idx = grid_idx[:, 0, 0, 0]  # (chunk_size,)
+        elif len(grid_idx.shape) == 3:
+            grid_idx = grid_idx[:, 0, 0]  # (chunk_size,)
+        elif len(grid_idx.shape) == 2:
+            grid_idx = grid_idx[:, 0]  # (chunk_size,)
+        else:
+            raise ValueError(
+                "The input to bilateral grid slicing is not supported yet."
+            )
+
+    affine_mats = bil_grids(xy, rgb, grid_idx)
+    rgb = color_affine_transform(affine_mats, rgb)
+
+    return {
+        "rgb": rgb.reshape(*sh_),
+        "rgb_affine_mats": affine_mats.reshape(
+            *sh_[:-1], affine_mats.shape[-2], affine_mats.shape[-1]
+        ),
+    }
+
+
+class BilateralGrid(nn.Module):
+    """Class for 3D bilateral grids.
+
+    Holds one or more than one bilateral grids.
+    """
+
+    def __init__(self, num, grid_X=16, grid_Y=16, grid_W=8):
+        """
+        Args:
+            num (int): The number of bilateral grids (i.e., the number of views).
+            grid_X (int): Defines grid width $W$.
+            grid_Y (int): Defines grid height $H$.
+            grid_W (int): Defines grid guidance dimension $L$.
+        """
+        super(BilateralGrid, self).__init__()
+
+        self.grid_width = grid_X
+        """Grid width. Type: int."""
+        self.grid_height = grid_Y
+        """Grid height. Type: int."""
+        self.grid_guidance = grid_W
+        """Grid guidance dimension. Type: int."""
+
+        # Initialize grids.
+        grid = self._init_identity_grid()
+        self.grids = nn.Parameter(grid.tile(num, 1, 1, 1, 1))  # (N, 12, L, H, W)
+        """ A 5-D tensor of shape $(N, 12, L, H, W)$."""
+
+        # Weights of BT601 RGB-to-gray.
+        self.register_buffer("rgb2gray_weight", torch.Tensor([[0.299, 0.587, 0.114]]))
+        self.rgb2gray = lambda rgb: (rgb @ self.rgb2gray_weight.T) * 2.0 - 1.0
+        """ A function that converts RGB to gray-scale guidance in $[-1, 1]$."""
+
+    def _init_identity_grid(self):
+        grid = torch.tensor(
+            [
+                1.0,
+                0,
+                0,
+                0,
+                0,
+                1.0,
+                0,
+                0,
+                0,
+                0,
+                1.0,
+                0,
+            ]
+        ).float()
+        grid = grid.repeat(
+            [self.grid_guidance * self.grid_height * self.grid_width, 1]
+        )  # (L * H * W, 12)
+        grid = grid.reshape(
+            1, self.grid_guidance, self.grid_height, self.grid_width, -1
+        )  # (1, L, H, W, 12)
+        grid = grid.permute(0, 4, 1, 2, 3)  # (1, 12, L, H, W)
+        return grid
+
+    def tv_loss(self):
+        """Computes and returns total variation loss on the bilateral grids."""
+        return total_variation_loss(self.grids)
+
+    def forward(self, grid_xy, rgb, idx=None):
+        """Bilateral grid slicing. Supports 2-D, 3-D, 4-D, and 5-D input.
+        For the 2-D, 3-D, and 4-D cases, please refer to `slice`.
+        For the 5-D cases, `idx` will be unused and the first dimension of `xy` should be
+        equal to the number of bilateral grids. Then this function becomes PyTorch's
+        [`F.grid_sample`](https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html).
+
+        Args:
+            grid_xy (torch.Tensor): The x-y coordinates in the range of $[0,1]$.
+            rgb (torch.Tensor): The RGB values in the range of $[0,1]$.
+            idx (torch.Tensor): The bilateral grid indices.
+
+        Returns:
+            Sliced affine matrices of shape $(..., 3, 4)$.
+        """
+
+        grids = self.grids
+        input_ndims = len(grid_xy.shape)
+        assert len(rgb.shape) == input_ndims
+
+        if input_ndims > 1 and input_ndims < 5:
+            # Convert input into 5D
+            for i in range(5 - input_ndims):
+                grid_xy = grid_xy.unsqueeze(1)
+                rgb = rgb.unsqueeze(1)
+            assert idx is not None
+        elif input_ndims != 5:
+            raise ValueError(
+                "Bilateral grid slicing only takes either 2D, 3D, 4D and 5D inputs"
+            )
+
+        grids = self.grids
+        if idx is not None:
+            grids = grids[idx]
+        assert grids.shape[0] == grid_xy.shape[0]
+
+        # Generate slicing coordinates.
+        grid_xy = (grid_xy - 0.5) * 2  # Rescale to [-1, 1].
+        grid_z = self.rgb2gray(rgb)
+
+        # print(grid_xy.shape, grid_z.shape)
+        # exit()
+        grid_xyz = torch.cat([grid_xy, grid_z], dim=-1)  # (N, m, h, w, 3)
+
+        affine_mats = F.grid_sample(
+            grids, grid_xyz, mode="bilinear", align_corners=True, padding_mode="border"
+        )  # (N, 12, m, h, w)
+        affine_mats = affine_mats.permute(0, 2, 3, 4, 1)  # (N, m, h, w, 12)
+        affine_mats = affine_mats.reshape(
+            *affine_mats.shape[:-1], 3, 4
+        )  # (N, m, h, w, 3, 4)
+
+        for _ in range(5 - input_ndims):
+            affine_mats = affine_mats.squeeze(1)
+
+        return affine_mats
diff --git a/threedgut_tracer/tracer.py b/threedgut_tracer/tracer.py
index 0377bd0..36fce98 100644
--- a/threedgut_tracer/tracer.py
+++ b/threedgut_tracer/tracer.py
@@ -334,6 +334,30 @@ class Tracer:
             pred_dist = pred_dist.unsqueeze(0).contiguous()
             hits_count = hits_count.unsqueeze(0).contiguous()
 
+            # --------------------------------------------------
+            # Bilateral grid color correction (before background)
+            # --------------------------------------------------
+            if hasattr(gaussians, "bil_grids") and gaussians.bil_grids is not None and hasattr(gpu_batch, "image_id") and gpu_batch.image_id is not None:
+                B, H, W, _ = pred_rgb.shape  # B==1 for our renderer
+                # Normalised pixel positions as in reference implementation
+                grid_y, grid_x = torch.meshgrid(
+                    (torch.arange(H, device=pred_rgb.device, dtype=pred_rgb.dtype) + 0.5) / H,
+                    (torch.arange(W, device=pred_rgb.device, dtype=pred_rgb.dtype) + 0.5) / W,
+                    indexing="ij",
+                )
+                grid_xy = torch.stack([grid_x, grid_y], dim=-1).unsqueeze(0)  # [1,H,W,2]
+
+                img_ids = gpu_batch.image_id
+                if img_ids.ndim == 1 or (img_ids.ndim == 2 and img_ids.shape[1] == 1):
+                    img_ids = img_ids.view(1, 1, 1, 1).expand(-1, H, W, 1)
+                else:
+                    img_ids = img_ids.expand(1, H, W, 1)
+
+                from threedgrut.utils.lib_bilagrid import slice as bilagrid_slice
+                guidance_rgb = torch.clamp(pred_rgb, 0.0, 1.0)
+                slice_out = bilagrid_slice(gaussians.bil_grids, grid_xy, guidance_rgb, img_ids)
+                pred_rgb = torch.clamp(slice_out["rgb"], 0.0, 1.0)
+
             pred_rgb, pred_opacity = gaussians.background(
                 gpu_batch.T_to_world.contiguous(), rays_d, pred_rgb, pred_opacity, train
             )
diff --git a/train_greentrees.sh b/train_greentrees.sh
new file mode 100755
index 0000000..021a7da
--- /dev/null